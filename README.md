# ğŸš€ Offline AI Chat â€“ Your Local ChatGPT Alternative

**Run Llama models 100% offline with a sleek web UI.** No cloud, no Docker bloat, no internet after setup. Perfect for privacy pros, devs, and offline warriors.

## ğŸ¥ Demo Video

[![Watch Demo](https://github.com/Adityareddy4310/offline-ai-chat/raw/main/thumbnail.png)](https://user-images.githubusercontent.com/12345678/98765432/11223344/demo%20video.mp4)

*Watch the full setup & offline chat demo â€“ 100% local!*

> ğŸ’¡ **Quick Win**: Get chatting in 2 mins. Fork this repo, star it â­, and share your custom model tweaks!

---

## âœ¨ Why This Project Rocks
Tired of Docker eating your RAM or cloud APIs spying on your prompts? This setup delivers **fast, private AI** on any machine. Built on Ollama's rock-solid local inference + Open-WebUI's intuitive chat.

- **ğŸ”’ Ultra-Private**: Everything stays on your device â€“ zero data leaks.
- **âš¡ Blazing Fast**: Starts in seconds, no VM overhead.
- **ğŸ§© Extensible**: Swap models, add RAG, or integrate tools like LangChain.
- **ğŸŒ Cross-Platform**: Windows/Linux/macOS ready (Windows-native focus).
- **ğŸ“± Offline-First**: Pull once, chat forever â€“ even on a plane.

> Inspired by [Ollama](https://ollama.com) and [Open-WebUI](https://openwebui.com). Join the local AI revolution!

---

## ğŸ›  Quick Start â€“ From Zero to Chat in 2 Mins

### 1. Grab Ollama (The Brain)
Download the native installer:  
[Windows](https://ollama.com/download/windows) | [macOS](https://ollama.com/download/macos) | [Linux](https://ollama.com/download/linux)

### 2. Pull Your First Model (One-Time Magic)
Open terminal/cmd and run:
```bash
ollama pull llama3.1:8b
```
*Test it*: `ollama run llama3.1:8b` â†’ Ask: "What's the meaning of life?" â†’ `/bye` to exit.

### 3. Fire Up the UI (Python Power)
```bash
# Create project space
mkdir offline-ai-chat && cd offline-ai-chat

# Set up venv (isolated & clean)
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or on Windows: venv\Scripts\activate

# Install & launch
pip install --upgrade pip
pip install open-webui
open-webui serve --port 3000
```

Boom! Head to [http://localhost:3000](http://localhost:3000) â†’ Select your model â†’ Chat away!

> âš ï¸ **Pro Tip**: For offline model glitches, hit Settings > Connections > Test Ollama link. Fixed in seconds!

### Docker Lovers? (Optional, Low-Overhead)
```bash
docker run -d -p 3000:8080 -v open-webui:/app/backend/data --add-host=host.docker.internal:host-gateway -e WEBUI_AUTH=False ghcr.io/open-webui/open-webui:main
```

---

## ğŸ¤– Model Showdown â€“ Pick Your Power Level

Compare models like a boss. All pull with `ollama pull [name]` â€“ choose based on your hardware!

| Model              | Size   | Speed ğŸš€ | Smarts ğŸ§  | Vibe                          | Pull Command                  |
|--------------------|--------|----------|-----------|-------------------------------|-------------------------------|
| **llama3.1:8b**   | 4.9 GB | â­â­â­â­  | â­â­â­â­â­ | Balanced chat king            | `ollama pull llama3.1:8b`    |
| **llama3.2:3b**   | 2.0 GB | â­â­â­â­â­| â­â­â­â­  | Speedy for low-RAM laptops    | `ollama pull llama3.2:3b`    |
| **mistral:7b**    | 4.1 GB | â­â­â­â­  | â­â­â­â­â­ | Creative & witty responses    | `ollama pull mistral:7b`     |
| **phi3:medium**   | 2.3 GB | â­â­â­â­â­| â­â­â­â­  | Code/debugging whiz           | `ollama pull phi3:medium`    |

> ğŸ” **Try This**: Start with llama3.1:8b, then remix with a custom Modelfile for your style (e.g., "Act like a pirate coder").

---

## ğŸ® Hands-On Demos â€“ See It in Action

Beyond the video thumbnail above, here's what you'll build:

- **Basic Chat**: Prompt: "Plan a vegan dinner party for 4 â€“ keep it under $50." â†’ Gets a full menu + shopping list.
- **Reasoning Test**: "Solve: Bat + ball = $1.10, bat $1 more than ball?" â†’ Step-by-step logic (spoiler: $0.05!).
- **Creative Spark**: "Write a haiku about quantum cats." â†’ Poetic AI magic.

Upload your own demo GIFs/PRs to `/demos/` â€“ let's crowdsource epic examples!

---

## ğŸ§‘â€ğŸ’» Project Structure (Keep It Tidy)
```
offline-ai-chat/
â”œâ”€â”€ demo video.mp4      # Your full setup walkthrough
â”œâ”€â”€ thumbnail.png       # Clickable preview (grab a frame from your vid!)
â”œâ”€â”€ README.md           # This file â€“ your project's front door
â”œâ”€â”€ setup-notes.md      # Custom tweaks & troubleshooting
â””â”€â”€ .gitignore          # Ignore venv & temp files
```

> ğŸ“‚ **Hack It**: Add a `scripts/` folder for auto-launch (e.g., PowerShell one-liner). PRs welcome!

---

## ğŸš§ Troubleshooting â€“ Common Wins
- **Models Vanish Offline?** Refresh Settings > Ollama > Test Connection. (GitHub issue #1 vibes â€“ fixed!)
- **Slow on CPU?** Drop to 3B model or enable GPU (NVIDIA/AMD auto-detects).
- **Port Clash?** Swap `--port 3000` to 8080.
- **Stuck?** Ping me in Issues or join [Ollama Discord](https://discord.gg/ollama).

---

## ğŸŒŸ Join the Fun â€“ Let's Build Together!
- â­ **Star** this repo to fuel the local AI fire.
- ğŸ´ **Fork** & tweak â€“ add voice mode? RAG docs? Your call!
- ğŸ’¬ **Chat** in Issues/Discussions â€“ share your wildest prompts.
- ğŸ¤ **Contribute**: Docs, models, or UI skins. See [CONTRIBUTING.md](CONTRIBUTING.md) for the lowdown.

> ğŸ’­ **What's Next?** Roadmap: Auto-model switching, mobile PWA support. Vote in Issues!

**Made with â¤ï¸ for offline dreamers. License: MIT â€“ Use freely, credit kindly.**

---

[Ollama Docs](https://ollama.com/docs) | [Open-WebUI Guide](https://docs.openwebui.com) | [Your Feedback Here](https://github.com/Adityareddy4310/offline-ai-chat/issues/new)
````

---
