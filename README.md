# ğŸ“Œ offline-ai-chat

A fully offline, self-hosted AI chat system using **Ollama**, **Open WebUI**, and **Llama 3.1**.  
This project demonstrates how to run a modern LLM completely **locally**, without any cloud or API dependencies â€” ideal for secure or offline environments.

---

## ğŸ¥ Demo Video

Since GitHub does not support playing MP4 directly inside a README,  
click the image below to watch the full demo:

[![Demo Video](thumbnail.png)](https://raw.githubusercontent.com/Adityareddy4310/offline-ai-chat/main/demo%20video.mp4)

> ğŸ”¹ Replace `thumbnail.png` with any screenshot from your demo video.  
> Upload a PNG/JPG image and rename it to **thumbnail.png**.

---

## ğŸš€ Overview

**offline-ai-chat** allows you to run Llama models completely offline:

- No API keys  
- No cloud  
- No data leaving your machine  
- Fast, private inference  
- Clean web UI via Open WebUI  

---

## âœ¨ Features

- ğŸ§  Run Llama 3.1 fully offline  
- ğŸŒ Chat UI with Open WebUI  
- ğŸ”’ Privacy-first (no internet required)  
- âš¡ Fast inference via Ollama backend  
- ğŸ–¥ï¸ Works on Windows / Linux / macOS  
- ğŸ”Œ Simple setup, easy to extend  

---

## ğŸ§± Tech Stack

| Tool | Purpose |
|------|---------|
| **Ollama** | Local model runtime |
| **Open WebUI** | Browser-based chat interface |
| **Docker** | Containerized deployment |
| **Llama 3.1 (8B)** | Main model used |

---

## ğŸ›  Installation & Setup

### 1ï¸âƒ£ Install Ollama  
Download Ollama from: https://ollama.com/download

### 2ï¸âƒ£ Pull the model  
```bash
ollama pull llama3.1
3ï¸âƒ£ Run Open WebUI
bash
Copy code
docker run -d \
  -p 3000:8080 \
  -v open-webui:/app \
  --name open-webui \
  ghcr.io/open-webui/open-webui:main
4ï¸âƒ£ Open the UI
Visit:

arduino
Copy code
http://localhost:3000
ğŸ“‚ Project Structure
bash
Copy code
offline-ai-chat/
â”œâ”€â”€ demo video.mp4    # Demo video
â”œâ”€â”€ thumbnail.png      # Thumbnail image used in README
â”œâ”€â”€ README.md
ğŸ¤– How It Works
Ollama hosts the model locally

Open WebUI connects to Ollama

You chat through the browser

All processing happens on your device

ğŸ“Œ Use Cases
Private AI assistant

Offline labs

Secure enterprise setups

Machine learning practice

Research experiments

ğŸ¤ Contributing
Pull requests are welcome!
Feel free to improve documentation, add features, or optimize the setup.
